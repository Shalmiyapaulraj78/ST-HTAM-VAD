{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5650e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_sequential_frames(base_path, category, num_frames=10, subcategory=None, overlap=True, skip_x = 1):\n",
    "    if subcategory:\n",
    "        path = os.path.join(base_path, category, subcategory)\n",
    "    else:\n",
    "        path = os.path.join(base_path, category)\n",
    "    \n",
    "    filenames = [os.path.join(path, f) for f in sorted(os.listdir(path), key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))]\n",
    "    \n",
    "    # Group by video source\n",
    "    grouped_filenames = {}\n",
    "    for file in filenames:\n",
    "        video_source = re.match(r'^(.*_x264)_', os.path.basename(file)).group(1)\n",
    "        if video_source not in grouped_filenames:\n",
    "            grouped_filenames[video_source] = []\n",
    "        grouped_filenames[video_source].append(file)\n",
    "\n",
    "    sequences = []\n",
    "    for video, files in grouped_filenames.items():\n",
    "        if overlap:\n",
    "            for i in range(0, len(files) - num_frames + 1, skip_x):\n",
    "                sequences.append(files[i:i+num_frames])\n",
    "        else:\n",
    "            for i in range(0, len(files), num_frames):\n",
    "                if i+num_frames <= len(files):\n",
    "                    sequences.append(files[i:i+num_frames])\n",
    "            \n",
    "    return sequences\n",
    "\n",
    "base_path = \"dataset/Train/\"\n",
    "\n",
    "\n",
    "anomaly_categories = os.listdir(os.path.join(base_path, \"AnomalyVideos\"))\n",
    "anomaly_sequences = []\n",
    "for category in anomaly_categories:\n",
    "    anomaly_sequences.extend(extract_sequential_frames(base_path, \"AnomalyVideos\", 16,  category, overlap=True, skip_x=4))\n",
    "\n",
    "\n",
    "normal_sequences = extract_sequential_frames(base_path, \"NormalVideos\", 16, overlap=True, skip_x=4)\n",
    "\n",
    "# Printing results\n",
    "print(\"Sequences from AnomalyVideos:\")\n",
    "for seq in anomaly_sequences:\n",
    "    print(seq)\n",
    "\n",
    "print(\"\\nSequences from NormalVideos:\")\n",
    "for seq in normal_sequences:\n",
    "    print(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(anomaly_sequences), len(normal_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c78e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def save_to_pkl(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Split the sequences into train and test sets\n",
    "subset_size = 1000\n",
    "subset_size = min(subset_size, len(anomaly_sequences), len(normal_sequences))\n",
    "anomaly_train, anomaly_test = train_test_split(anomaly_sequences[:subset_size], test_size=0.2, random_state=42)\n",
    "normal_train, normal_test = train_test_split(normal_sequences[:subset_size], test_size=0.2, random_state=42)\n",
    "\n",
    "# Save lists to .pkl files\n",
    "save_to_pkl(anomaly_train, 'anomaly_train.pkl')\n",
    "save_to_pkl(anomaly_test, 'anomaly_test.pkl')\n",
    "save_to_pkl(normal_train, 'normal_train.pkl')\n",
    "save_to_pkl(normal_test, 'normal_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SwinModel\n",
    "import torch\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"swin-base-patch4-window7-224\")\n",
    "model = SwinModel.from_pretrained(\"swin-base-patch4-window7-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def save_swin_features(image_sequence, image_processor, model, data_fname):\n",
    "    img_list = [Image.open(img_path) for img_path in image_sequence]\n",
    "    #print(len(img_list)) # 10\n",
    "\n",
    "    inputs = image_processor(img_list, return_tensors=\"pt\")         \n",
    "    #print(inputs.pixel_values.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state              \n",
    "    #print(last_hidden_states.shape)\n",
    "\n",
    "    torch.save(last_hidden_states, data_fname)\n",
    "    return last_hidden_states\n",
    "\n",
    "\n",
    "for i, sequence in enumerate(anomaly_train):\n",
    "    save_swin_features(sequence, image_processor, model, f\"swin_embeddings_mini/train/1/1_{i}.pt\")\n",
    "for i, sequence in enumerate(anomaly_test):\n",
    "    save_swin_features(sequence, image_processor, model, f\"swin_embeddings_mini/test/1/1_{i}.pt\")\n",
    "\n",
    "for i, sequence in enumerate(normal_train):\n",
    "    save_swin_features(sequence, image_processor, model, f\"swin_embeddings_mini/train/0/0_{i}.pt\")\n",
    "for i, sequence in enumerate(normal_test):\n",
    "    save_swin_features(sequence, image_processor, model, f\"swin_embeddings_mini/test/0/0_{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135dadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "from swin_functions_and_classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7668be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_features/Anomaly_Swin\"\n",
    "\n",
    "image_path = \"Test/AnomalyVideos\"\n",
    "for subfile in os.listdir(image_path):\n",
    "    subfile_path = os.path.join(image_path, subfile)\n",
    "    for subsubfile in os.listdir(subfile_path):\n",
    "        hdf5_filename = os.path.join(output_dir, f\"features_{subfile}_{subsubfile}.h5\")\n",
    "        hdf5_file = h5py.File(hdf5_filename, \"w\")\n",
    "        subsubfile_path = os.path.join(subfile_path, subsubfile)\n",
    "        image = Image.open(subsubfile_path)\n",
    "        transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        tensor_image = transform(image)\n",
    "        tensor_image = tensor_image.unsqueeze(0)\n",
    "        \n",
    "        patch_embed = PatchEmbed(img_size=224, patch_size=4, in_chans=3, embed_dim=96)\n",
    "        embedding = patch_embed(tensor_image)\n",
    "        \n",
    "        stage1 = BasicLayer(dim=96, input_resolution=(56,56), depth=2, num_heads=4, window_size=7)\n",
    "        output = stage1(embedding)\n",
    "        \n",
    "        merge_layer = PatchMerging(input_resolution=(56,56), dim=96, norm_layer=nn.LayerNorm)\n",
    "        merged_output = merge_layer(output)\n",
    "        \n",
    "        block_1 = SwinTransformerBlock(dim=96, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        \n",
    "        block_1_shf = SwinTransformerBlock(dim=96, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=2, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        \n",
    "        output = block_1(embedding)\n",
    "        output_shf = block_1_shf(output)\n",
    "        \n",
    "        output_tensor = output_shf.detach().cpu()\n",
    "        output_array = output_tensor.cpu().numpy()\n",
    "        \n",
    "        dataset_name = \"output_data\"\n",
    "        hdf5_file.create_dataset(dataset_name, data=output_array)\n",
    "        \n",
    "        hdf5_file.close()\n",
    "print(\"All spatial features saved to HDF5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fea931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5_filename = \"output_features/Anomaly_Swin/features_Anomaly_Swin.h5\"\n",
    "hdf5_file = h5py.File(hdf5_filename, \"r\")\n",
    "output_data = hdf5_file[\"output_data\"]\n",
    "output_array = output_data[:]\n",
    "\n",
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output_features/Normal_Swin\"\n",
    "\n",
    "image_path = \"Test/NormalVideos\"\n",
    "for subfile in os.listdir(image_path):\n",
    "        subfile_path = os.path.join(image_path, subfile)\n",
    "        hdf5_filename = os.path.join(output_dir, f\"features_{subfile}.h5\")\n",
    "        hdf5_file = h5py.File(hdf5_filename, \"w\")\n",
    "    #for subsubfile in os.listdir(subfile_path):\n",
    "        #subsubfile_path = os.path.join(subfile_path, subsubfile)\n",
    "        image = Image.open(subfile_path)\n",
    "        transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "        tensor_image = transform(image)\n",
    "        tensor_image = tensor_image.unsqueeze(0)\n",
    "        \n",
    "        patch_embed = PatchEmbed(img_size=224, patch_size=4, in_chans=3, embed_dim=96)\n",
    "        embedding = patch_embed(tensor_image)\n",
    "        \n",
    "        stage1 = BasicLayer(dim=96, input_resolution=(56,56), depth=2, num_heads=4, window_size=7)\n",
    "        output = stage1(embedding)\n",
    "        \n",
    "        merge_layer = PatchMerging(input_resolution=(56,56), dim=96, norm_layer=nn.LayerNorm)\n",
    "        merged_output = merge_layer(output)\n",
    "        \n",
    "        block_1 = SwinTransformerBlock(dim=96, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        \n",
    "        block_1_shf = SwinTransformerBlock(dim=96, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=2, mlp_ratio=4.0, qkv_bias=True, qk_scale=None, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        \n",
    "        output = block_1(embedding)\n",
    "        output_shf = block_1_shf(output)\n",
    "        \n",
    "        output_tensor = output_shf.detach().cpu()\n",
    "        output_array = output_tensor.cpu().numpy()\n",
    "        \n",
    "        dataset_name = \"output_data\"\n",
    "        hdf5_file.create_dataset(dataset_name, data=output_array)\n",
    "        \n",
    "        hdf5_file.close()\n",
    "print(\"All spatial features saved to HDF5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_testing = \"Test/AnomalyVideos\"\n",
    "for subfile in os.listdir(image_path_testing):\n",
    "    subfile_path_testing = os.path.join(image_path_testing, subfile)\n",
    "    for subsubfile in os.listdir(subfile_path_testing):\n",
    "        print(subsubfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
