{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6617725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch and general Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_from_pkl(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load lists from .pkl files\n",
    "anomaly_sequences = load_from_pkl('anomaly_train.pkl')\n",
    "normal_sequences = load_from_pkl('normal_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70310489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Globalselfatt(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(Globalselfatt, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Define convolution layers\n",
    "        self.theta = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "        self.phi = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "        self.b = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute theta, phi, and b\n",
    "        theta = self.theta(x)\n",
    "        phi = self.phi(x)\n",
    "        b = self.b(x)\n",
    "\n",
    "        # Transpose theta\n",
    "        theta = theta.permute(0, 2, 1)\n",
    "\n",
    "        # Compute the attention map using matrix multiplication and softmax\n",
    "        attention = F.softmax(torch.matmul(theta, phi), dim=-1)\n",
    "\n",
    "        # Compute the output using the attention map\n",
    "        output = torch.matmul(b, attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e93117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTAMModule(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size):\n",
    "        super(HTAMModule, self).__init__()\n",
    "        \n",
    "        # Conv1D layer\n",
    "        self.conv1d = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "\n",
    "        self.nln_block = Globalselfatt(out_channels=64)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=64*1022, hidden_size=hidden_size, batch_first=True, bidirectional=False)       #, batch_first=True\n",
    "\n",
    "    def forward(self, frame_features):\n",
    "        #print(\"frame\",frame_features.shape)\n",
    "        \n",
    "        # Apply Conv1D layer\n",
    "        conv1d_features = self.conv1d(frame_features)\n",
    "        #print(\"conv1\",conv1d_features.shape)\n",
    "\n",
    "        conv1d_features = self.relu(conv1d_features)\n",
    "        #print(\"conv1\",conv1d_features.shape)\n",
    "        \n",
    "        # Apply Globalselfatt block\n",
    "        nln_features = self.nln_block(conv1d_features)\n",
    "        #print(\"nln\",nln_features.shape)\n",
    "\n",
    "        flattened_nln_features = nln_features.reshape(10, -1)\n",
    "        #print(\"flat nln\", flattened_nln_features.shape)\n",
    "        \n",
    "        # Apply LSTM layer\n",
    "        lstm_features, _ = self.lstm(flattened_nln_features)\n",
    "        \n",
    "        return lstm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size2,hidden_size3,  output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.lyrs = [self.fc]\n",
    "\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for lyr in self.lyrs:\n",
    "            torch.nn.init.xavier_uniform_(lyr.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu( self.fc(x) )\n",
    "        x = nn.Dropout(0.2)(x)\n",
    "        x = self.sigmoid(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d34375",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size2, hidden_size3, output_size, htam_input_channels, htam_hidden_size):\n",
    "        super(CombinedModel, self).__init__()\n",
    "\n",
    "        # HTAM Module\n",
    "        self.htam_module = HTAMModule(input_channels=htam_input_channels, hidden_size=htam_hidden_size)\n",
    "\n",
    "        # Classifier Module\n",
    "        self.classifier = Classifier(input_size, hidden_size, hidden_size2, hidden_size3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the HTAM Module\n",
    "        htam_output = self.htam_module(x)\n",
    "\n",
    "        # Pass the HTAM output through the Classifier\n",
    "        classifier_output = self.classifier(torch.flatten(htam_output))\n",
    "\n",
    "        return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "input_size = 16*512\n",
    "hidden_size = input_size // 2\n",
    "hidden_size2 = 512\n",
    "hidden_size3 = 32\n",
    "output_size = 1\n",
    "\n",
    "htam_input_channels = 49  \n",
    "htam_hidden_size = 512    \n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fa3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombinedModel(input_size, hidden_size, hidden_size2, hidden_size3, output_size, htam_input_channels, htam_hidden_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b572aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "test_inference = False\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = 0\n",
    "\n",
    "    for i in range(0, len(anomaly_sequences), batch_size):\n",
    "        \n",
    "        # Anomaly sequences\n",
    "        for j in range(i, min(i+batch_size, len(anomaly_sequences))):\n",
    "            #print(f\"anomaly {j}\")\n",
    "            fname =  f\"swin_embeddings/train/1/1_{j}.pt\"\n",
    "            try:\n",
    "                abnormal_spatial_embeddings = torch.load(fname)\n",
    "            except:\n",
    "                break\n",
    "            anomaly_output = model(abnormal_spatial_embeddings)\n",
    "            anomaly_label = torch.ones((1), dtype=torch.float32)\n",
    "            anomaly_loss = criterion(anomaly_output, anomaly_label)\n",
    "            batch_loss += anomaly_loss\n",
    "\n",
    "    for i in range(0, len(normal_sequences), batch_size):\n",
    "\n",
    "        # Normal sequences\n",
    "        for j in range(i, min(i+batch_size, len(normal_sequences))):\n",
    "            #print(f\"normal {j}\")\n",
    "            fname =  f\"swin_embeddings/train/0/0_{j}.pt\"\n",
    "            try:\n",
    "                normal_spatial_embeddings = torch.load(fname)\n",
    "            except:\n",
    "                break\n",
    "            normal_output = model(normal_spatial_embeddings)\n",
    "            normal_label = torch.zeros((1), dtype=torch.float32)\n",
    "            normal_loss = criterion(normal_output, normal_label)\n",
    "            batch_loss += normal_loss\n",
    "\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if test_inference:\n",
    "        precision, recall, fpr, tpr, auc, accuracy = inference(model)\n",
    "        history.append([batch_loss.item(), precision, recall, fpr, tpr, auc])\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {batch_loss.item()}, Accuracy:{accuracy}, Precision: {precision}, Recall: {recall}, AUC: {auc}')\n",
    "    else:\n",
    "        history.append([batch_loss.item()])\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {batch_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a416f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def inference(model):\n",
    "    model.eval()  \n",
    "\n",
    "    anomaly_test_data = load_from_pkl('anomaly_test.pkl')\n",
    "    normal_test_data = load_from_pkl('normal_test.pkl')\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # For anomaly data\n",
    "        for i in range(len(anomaly_test_data)):\n",
    "            fname =  f\"swin_embeddings/test/1/1_{i}.pt\"\n",
    "            data = torch.load(fname)\n",
    "            output = model(data)\n",
    "            prediction = (output > 0.5).float().item()\n",
    "            all_predictions.append(prediction)\n",
    "            all_labels.append(1.0)  \n",
    "\n",
    "        # For normal data\n",
    "        for i in range(len(normal_test_data)):\n",
    "            fname =  f\"swin_embeddings/test/0/0_{i}.pt\"\n",
    "            data = torch.load(fname)\n",
    "            output = model(data)\n",
    "            prediction = (output > 0.5).float().item()\n",
    "            all_predictions.append(prediction)\n",
    "            all_labels.append(0.0)  \n",
    "\n",
    "    # Calculate metrics\n",
    "    #print(all_labels)\n",
    "    #print(all_predictions)\n",
    "    #print(len(all_labels))\n",
    "    labels = \"Test/GroundTruth\"\n",
    "    pred = \"Test/predictedValues\"\n",
    "    labels_roc = \"Test/predictedLabels\"\n",
    "    pred_roc = \"Test/PredictedROC\"\n",
    "    print(len(labels))\n",
    "    precision = precision_score(all_labels, pred)\n",
    "    recall = recall_score(all_labels, pred)\n",
    "    accu = accuracy_score(all_labels, pred)\n",
    "    \n",
    "    \n",
    "    all_scores = [output for output in fake_pred_roc]\n",
    "    fpr, tpr, thresholds = roc_curve(fake_labels_roc, all_scores)\n",
    "    auc = roc_auc_score(fake_labels_roc, all_scores)\n",
    "\n",
    "    return precision, recall, fpr, tpr, auc, accu   # Return the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba60d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "test_inference = False\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = 0\n",
    "\n",
    "    for i in range(0, len(anomaly_sequences), batch_size):\n",
    "        \n",
    "        # Anomaly sequences\n",
    "        for j in range(i, min(i+batch_size, len(anomaly_sequences))):\n",
    "            #print(f\"anomaly {j}\")\n",
    "            fname =  f\"swin_embeddings/train/1/1_{j}.pt\"\n",
    "            try:\n",
    "                abnormal_spatial_embeddings = torch.load(fname)\n",
    "            except:\n",
    "                break\n",
    "            anomaly_output = model(abnormal_spatial_embeddings)\n",
    "            anomaly_label = torch.ones((1), dtype=torch.float32)\n",
    "            anomaly_loss = criterion(anomaly_output, anomaly_label)\n",
    "            batch_loss += anomaly_loss\n",
    "\n",
    "    for i in range(0, len(normal_sequences), batch_size):\n",
    "\n",
    "        # Normal sequences\n",
    "        for j in range(i, min(i+batch_size, len(normal_sequences))):\n",
    "            #print(f\"normal {j}\")\n",
    "            fname =  f\"swin_embeddings/train/0/0_{j}.pt\"\n",
    "            try:\n",
    "                normal_spatial_embeddings = torch.load(fname)\n",
    "            except:\n",
    "                break\n",
    "            normal_output = model(normal_spatial_embeddings)\n",
    "            normal_label = torch.zeros((1), dtype=torch.float32)\n",
    "            normal_loss = criterion(normal_output, normal_label)\n",
    "            batch_loss += normal_loss\n",
    "\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if test_inference:\n",
    "        precision, recall, fpr, tpr, auc, accuracy = inference(model)\n",
    "        history.append([batch_loss.item(), precision, recall, fpr, tpr, auc])\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {batch_loss.item()}, Accuracy:{accuracy}, Precision: {precision}, Recall: {recall}, AUC: {auc}')\n",
    "    else:\n",
    "        history.append([batch_loss.item()])\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {batch_loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
